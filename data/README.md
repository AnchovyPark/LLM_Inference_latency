# LLM Performance Delay Factor Analysis - Final Clean Data

ì´ í´ë”ëŠ” LLM ì„±ëŠ¥ ë¶„ì„ì„ ìœ„í•œ ìµœì¢… ì •ë¦¬ëœ ë°ì´í„°ì™€ ì½”ë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ“Š GPU ì„±ëŠ¥ ë°ì´í„° (Delay Factors)

ëª¨ë“  delay factorëŠ” **memory-bound ì—°ì‚°ì€ memory bandwidth ê¸°ì¤€**, **compute-bound ì—°ì‚°ì€ Peak TFLOPS ê¸°ì¤€**ìœ¼ë¡œ ì •í™•íˆ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.

### GPU Delay Factor Files:
- `T4_DF_memory_corrected.json` - Tesla T4 ì„±ëŠ¥ ë°ì´í„°
- `A10G_DF_memory_corrected.json` - NVIDIA A10G ì„±ëŠ¥ ë°ì´í„°  
- `L40s_DF_memory_corrected.json` - NVIDIA L40S ì„±ëŠ¥ ë°ì´í„°
- `A100_40G_DF_memory_corrected.json` - NVIDIA A100 40GB ì„±ëŠ¥ ë°ì´í„°
- `A100_80G_DF_memory_corrected.json` - NVIDIA A100 80GB ì„±ëŠ¥ ë°ì´í„°

### Delay Factor ë²”ìœ„ (ì •ìƒì ì¸ ê°’ë“¤):
- **Compute-bound ì—°ì‚°** (Linear_GEMM, BMM, GQA, SwiGLU_MLP): 1.0x - 7.0x
- **Memory-bound ì—°ì‚°** (Elementwise_Add, RMS_Norm, Softmax): 1.0x - 17.0x

## ğŸ¤– ëª¨ë¸ Configuration Files

- `llama_3_2_1b_config.json` - LLaMA 3.2 1B ëª¨ë¸ ì„¤ì •
- `llama_3_8b_config.json` - LLaMA 3 8B ëª¨ë¸ ì„¤ì •  
- `llama_3_70b_config.json` - LLaMA 3 70B ëª¨ë¸ ì„¤ì •

## ğŸ”§ í•µì‹¬ ì½”ë“œ íŒŒì¼ë“¤

### `gpu_specs_database.py`
- GPU í•˜ë“œì›¨ì–´ ìŠ¤í™ ë°ì´í„°ë² ì´ìŠ¤
- ìë™ GPU ê°ì§€ ê¸°ëŠ¥
- ì§€ì› GPU: T4, A10G, L40S, A100, V100, RTX 4090 ë“±

### `llm_latency_predictor.py`
- **ë©”ì¸ ì˜ˆì¸¡ ì‹œìŠ¤í…œ** ğŸ¯
- ëª¨ë¸ + GPU + input/output length â†’ latency ì˜ˆì¸¡
- delay factor fine-tuning ì§€ì›
- PREFILL + DECODE ë‹¨ê³„ë³„ ë¶„ì„

## ğŸš€ ì‚¬ìš©ë²•

### 1. ê¸°ë³¸ latency ì˜ˆì¸¡:
```python
from llm_latency_predictor import LLMLatencyPredictor

predictor = LLMLatencyPredictor()

# LLaMA 3.2 1B + T4 + 512 input + 100 output tokens
result = predictor.predict_latency('LLaMA_3.2_1B', 'T4', 512, 100)
print(f"Total latency: {result.total_ms:.1f}ms")
```

### 2. Delay factor ì¡°ì •:
```python
# Linear ì—°ì‚°ì„ 20% ë¹ ë¥´ê²Œ, Attentionì„ 10% ëŠë¦¬ê²Œ ì¡°ì •
predictor.fine_tune_delay_factors({
    'T4_Linear_GEMM': 0.8,
    'T4_BMM': 1.1
})
```

### 3. ì§€ì›ë˜ëŠ” ëª¨ë¸:
- `'LLaMA_3.2_1B'` - 2048 hidden, 16 layers
- `'LLaMA_3_8B'` - 4096 hidden, 32 layers  
- `'LLaMA_3_70B'` - 8192 hidden, 80 layers

### 4. ì§€ì›ë˜ëŠ” GPU:
- `'T4'`, `'A10G'`, `'L40S'`, `'A100'`

## ğŸ“ˆ ì„±ëŠ¥ íŠ¹ì„± ìš”ì•½

### GPU ì„±ëŠ¥ ìˆœìœ„ (ì „ì²´ í‰ê·  delay factor ê¸°ì¤€):
1. **L40S**: ê°€ì¥ íš¨ìœ¨ì  (1.0x-6.0x ë²”ìœ„)
2. **A100**: ê³ ì„±ëŠ¥ ë²”ìš© GPU (1.5x-17x ë²”ìœ„)  
3. **A10G**: ì¤‘ê¸‰ ì„±ëŠ¥ (2.0x-7.0x ë²”ìœ„)
4. **T4**: ê²½ì œì  ì˜µì…˜ (2.0x-7.0x ë²”ìœ„)

### ì—°ì‚°ë³„ íŠ¹ì„±:
- **Linear_GEMM**: ê°€ì¥ ìµœì í™”ëœ ì—°ì‚° (1-4x delay)
- **SwiGLU_MLP**: ë§¤ìš° íš¨ìœ¨ì  (1-2x delay)
- **BMM/Attention**: ì¤‘ê°„ ìˆ˜ì¤€ (3-7x delay)  
- **Memory-bound ì—°ì‚°**: ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ delay (2-17x)

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì¶”ê°€ ì˜¤ë²„í—¤ë“œ ë°œìƒ ê°€ëŠ¥**: ë©”ëª¨ë¦¬ ì „ì†¡, ì»¤ë„ ëŸ°ì¹˜ ë“±
2. **Batch size 1 ê¸°ì¤€**: ë” í° batch sizeì—ì„œëŠ” íš¨ìœ¨ì„± í–¥ìƒ ê°€ëŠ¥
3. **FP16 ê¸°ì¤€ ì¸¡ì •**: FP32ë‚˜ ë‹¤ë¥¸ precisionì—ì„œëŠ” ë‹¤ë¥¸ ê²°ê³¼ ê°€ëŠ¥
4. **GPUë³„ ìµœì í™” ì°¨ì´**: ë“œë¼ì´ë²„ ë²„ì „, CUDA ë²„ì „ì— ë”°ë¼ ì°¨ì´ ê°€ëŠ¥

## ğŸ¯ ê¶Œì¥ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

- **ê°œë°œ/í…ŒìŠ¤íŒ…**: T4 (ì €ë¹„ìš©)
- **í”„ë¡œë•ì…˜ ì¶”ë¡ **: A10G, L40S (ì„±ëŠ¥/ë¹„ìš© ê· í˜•)  
- **ëŒ€ìš©ëŸ‰/ì—°êµ¬**: A100 (ìµœê³  ì„±ëŠ¥)
- **ëª¨ë¸ í¬ê¸°ë³„**: 1Bâ†’T4, 8Bâ†’A10G/L40S, 70Bâ†’A100

---
ğŸ“… ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2024-08-14
ğŸ”¬ ì¸¡ì • í™˜ê²½: FP16, Single GPU, Batch Size 1
âœ… ëª¨ë“  delay factor ê²€ì¦ ì™„ë£Œ